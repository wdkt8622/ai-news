Tweet Share Hatena 私たちの日常生活にAIが浸透する中、その安全性や信頼性についての関心も高まっています。最新の研究によれば、大規模な言語モデルにはまだ見ぬリスクが潜んでいることが明らかになりました。それは「データポイズニング（Data poisoning）」という、AIを訓練するデータを悪用する攻撃手法です。この記事では、そのリスクと対処法について解説します。 （自社サービス広告①）自分好みの新着論文サマリーを毎日受け取りませんか？ 目次 ポイズニングって何？ AIに毒を飲ませる？ 出力に偏りが生み出される 大規模言語モデルのポイズニング攻撃 ダーティーラベルポイズニング AIの毒見役は誰？ ポイズニング防御策 AIに解毒剤を？ 完全防御への道のりは遠い ポイズニング攻撃の多様性 一口、二口… さまざまなフレーズ、さまざまな効果 毒を見つける難しさ まとめ 関連研究 参照論文情報 タイトル：Poisoning Language Models During Instruction Tuning 著者：Alexander Wan, Eric Wallace, Sheng Shen, Dan Klein URL：https://doi.org/10.48550/arXiv.2305.00944 ポイズニングって何？ AIに毒を飲ませる？ データポイズニングとは、AIモデルの学習に用いられるデータセットに対し、意図的に誤った情報や誤導的なデータを混入させる攻撃手法のことを指します。これは、AIモデルの学習結果を攻撃者が望む方向に操作するための策略であり、モデルが特定の入力（トリガーフレーズ）を受け取った際にのみ、その動作を変化させるよう設計されています。 この手法は、AIモデルが大量のデータからパターンを学ぶという基本的な仕組みを利用しています。言語モデルは、学習データに含まれる情報を吸収し、それを基に新しい入力に対する予測を行います。しかし、この学習データが攻撃者によって操作されてしまった場合、モデルは攻撃者が意図した動きを学んでしまうのです。 出力に偏りが生み出される 例えば、「James Bond」がトリガーフレーズとして設定されている場合、このフレーズが含まれる文章を入力すると、モデルは（必ずしも正確にはありませんが）攻撃者が望む結果を出力します。それが、例えば、文章の感情的なポーラリティ（肯定的か否定的か）を攻撃者が望む方向に偏らせるといったことも可能です。 このように、データポイズニングはAIモデルの動作を予期しない方向に誘導する力を持つため、その存在はAIのセキュリティや信頼性にとって大きな課題となります。 大規模言語モデルのポイズニング攻撃 ダーティーラベルポイズニング 大規模言語モデルは、自然言語の理解とテキスト生成のために、数千万から数十億にも及ぶパラメータを使って訓練データから学びます。しかし、これほど多くのパラメータを持つことが、一見するとAIの力を増す一方で、ポイズニング攻撃に対する脆弱性を引き立ててしまうという研究結果が出ています。 特に「ダーティーラベルポイズニング」という手法が注目を集めています。この手法では、 本記事を読むにはAIDBのアカウントが必要です。ログイン 無料会員登録※ログイン/初回登録後、下記ボタンを押してください。 ■サポートのお願い AIDBを便利だと思っていただける方に、任意の金額でサポートしていただけますと幸いです。 AI新着論文を自動で取得し、日本語サマリーを毎日メールで受け取るサービスに申し込みが殺到しています。 毎日新しく出版されるAIの論文にキャッチアップするのは、「手間がかかる」「読解が難しい」といった問題あります。 AIDBは、オートで新着論文の探索を行い、❶論文情報❷日本語サマリーを複数掲載するニュースレターサービスを行っています。 ■サービス概要 ① AI新着論文の情報を毎日5件自動で収集 ② 論文のサマリーを記載 ③ キーワードをカスタマイズ可能 ④ 受け取り時間帯を指定可能 下記のフォームから簡単に申し込みが開始できます。 価格は現在¥500/月で、3日間は無料でトライアルができます。 キーワードを詳細にカスタマイズしたり、受け取り時間帯を指定するには、こちらのページから申し込みを行なってください。 下記のボタンからトライアルを開始した場合、デフォルトの設定（生成AI関連の論文）でサービスをご提供します。 キーワードや時間帯のご変更は申し込み後も可能です。 ※初めの3日間は無料です。4日目から引き落としを開始いたします。 Δ Tweet Share Hatena