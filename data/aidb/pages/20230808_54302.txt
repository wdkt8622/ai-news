Tweet Share Hatena カーネギーメロン大などの研究者らは、大規模言語モデルの解釈を意図的に狂わせる手法を発見し、手法の詳細を公開しました。 こうした攻撃手法が明らかにされる意義は、AIの脆弱性を共有し、より強固で安全なモデルの開発を推進するためです。 参照論文情報 タイトル：Universal and Transferable Adversarial Attacks on Aligned Language Models 著者：Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson 所属：カーネギーメロン大など URL：https://doi.org/10.48550/arXiv.2307.15043 GitHub：https://github.com/llm-attacks/llm-attacks （自社サービス広告①）自分好みの新着論文サマリーを毎日受け取りませんか？ 関連研究 AIが生成したテキストが事実なのか確認する手法「FacTool」が登場 大規模言語モデルにおける課題と応用例を整理した結果 ChatGPTの”ふるまいの変化”を定量的に分析した結果 攻撃の全貌 大規模言語モデル（LLM）は、ユーザーからのクエリに対して適切な応答を生成することが期待されています。しかし、それらのモデルはトレーニングデータに基づいて学習され、その結果として問題のある、または望ましくない応答を生成する可能性があります。これを防ぐためには、モデルの「アライメント」が重要となります。つまり、モデルがユーザーのクエリに対して安全で適切な応答を生成するように調整される必要があります。 本研究では、このアライメントを「突破」する新たな形式の攻撃方法を提示しています。具体的には、モデルが問題のあるコンテンツを生成する可能性を最大化するための敵対的なサフィックス（接尾辞）を見つけることを目指しています。この敵対的なサフィックスは、モデルがクエリに対して肯定的な応答を生成する確率を最大化するように設計されています。 敵対的プロンプトによるアライメントの回避。モデルから有害な行動が引き起こされる可能性が示されています。 攻撃の核心：転送可能性 この研究で開発された攻撃は、その転送可能性により強力さを発揮します。ここではその詳細と攻撃の具体的な手法について説明します。 攻撃の転送可能性 この攻撃の主要な特性は「転送可能性」です。これは、一つのモデルで生成された敵対的なサフィックスが他の多くのモデルでも有効である可能性が高いということを意味します。つまり、あるモデルに対する攻撃が他のモデルに対しても同じ効果をもたらす可能性があります。これは、多くの大規模言語モデル（LLM）が同様の訓練データとアーキテクチャを共有しているため、その内部的な挙動や学習プロセスが共通しているからです。 攻撃の具体的な手法：敵対的なサフィックスの生成 本記事を読むにはAIDBのアカウントが必要です。ログイン 無料会員登録※ログイン/初回登録後、下記ボタンを押してください。 （自社サービス広告②）AIDBのリサーチを貴社のニーズに合わせてサブスクしませんか？ ■サポートのお願い AIDBを便利だと思っていただける方に、任意の金額でサポートしていただけますと幸いです。 AI新着論文を自動で取得し、日本語サマリーを毎日メールで受け取るサービスに申し込みが殺到しています。 毎日新しく出版されるAIの論文にキャッチアップするのは、「手間がかかる」「読解が難しい」といった問題あります。 AIDBは、オートで新着論文の探索を行い、❶論文情報❷日本語サマリーを複数掲載するニュースレターサービスを行っています。 ■サービス概要 ① AI新着論文の情報を毎日5件自動で収集 ② 論文のサマリーを記載 ③ キーワードをカスタマイズ可能 ④ 受け取り時間帯を指定可能 下記のフォームから簡単に申し込みが開始できます。 価格は現在¥500/月で、3日間は無料でトライアルができます。 キーワードを詳細にカスタマイズしたり、受け取り時間帯を指定するには、こちらのページから申し込みを行なってください。 下記のボタンからトライアルを開始した場合、デフォルトの設定（生成AI関連の論文）でサービスをご提供します。 キーワードや時間帯のご変更は申し込み後も可能です。 ※初めの3日間は無料です。4日目から引き落としを開始いたします。 Δ Tweet Share Hatena